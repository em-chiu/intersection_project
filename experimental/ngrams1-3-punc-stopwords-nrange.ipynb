{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/em-chiu/intersection_project/blob/main/EC%20Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSgFvHzhSPNK",
    "outputId": "fe6a4603-759f-483e-b192-e891f58f83e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emilychiu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/emilychiu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/emilychiu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import urllib.request\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "nltk.download('averaged_perceptron_tagger') # to solve pos feature extract issue\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import pandas as pd # data processing\n",
    "import csv\n",
    "import sys\n",
    "import unidecode\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data to df, try1 works\n",
    "df = pd.read_csv('FTR_labels_copy.csv', header=None, encoding = \"ISO-8859-1\")\\\n",
    "        .drop(0, axis=1)\\\n",
    "        .rename(columns={1: 'tweet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tweet</td>\n",
       "      <td>label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elle ne devait pas dÃ©gager la nazi ??? https:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>je dois faire un oral en anglais sur Hitler et...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@ambrassmoi_ A Ã§a ð¤ de ressembler Ã  la cr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Par curiositÃ©, jâai voulu Ã©couter les dis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet      2\n",
       "0                                              tweet  label\n",
       "1  Elle ne devait pas dÃ©gager la nazi ??? https:...      1\n",
       "2  je dois faire un oral en anglais sur Hitler et...      0\n",
       "3  @ambrassmoi_ A Ã§a ð¤ de ressembler Ã  la cr...      0\n",
       "4  \"Par curiositÃ©, jâai voulu Ã©couter les dis...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop first row \n",
    "# by selecting all rows from first row onwards\n",
    "df = df.iloc[1: , :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2856, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view dimensions of dataset\n",
    "df.shape\n",
    "# 2857 instances and 2 attributes in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet', 'label'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename column names\n",
    "col_names = ['tweet', 'label']\n",
    "df.columns = col_names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elle ne devait pas dÃ©gager la nazi ??? https:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>je dois faire un oral en anglais sur Hitler et...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@ambrassmoi_ A Ã§a ð¤ de ressembler Ã  la cr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Par curiositÃ©, jâai voulu Ã©couter les dis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@tical10 @ExBarcaFan74523 @ConflitsFrance Mdr ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet label\n",
       "1  Elle ne devait pas dÃ©gager la nazi ??? https:...     1\n",
       "2  je dois faire un oral en anglais sur Hitler et...     0\n",
       "3  @ambrassmoi_ A Ã§a ð¤ de ressembler Ã  la cr...     0\n",
       "4  \"Par curiositÃ©, jâai voulu Ã©couter les dis...     0\n",
       "5  @tical10 @ExBarcaFan74523 @ConflitsFrance Mdr ...     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview after renaming\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2856 entries, 1 to 2856\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweet   2856 non-null   object\n",
      " 1   label   2856 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 44.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# summary of data\n",
    "df.info()\n",
    "# no missing data\n",
    "# Categorical variables have data type object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 categorical variables\n",
      "\n",
      "The categorical variables are :\n",
      "\n",
      " ['tweet', 'label']\n"
     ]
    }
   ],
   "source": [
    "# find categorical variables\n",
    "categorical = [var for var in df.columns if df[var].dtype=='O']\n",
    "\n",
    "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
    "\n",
    "print('The categorical variables are :\\n\\n', categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elle ne devait pas dÃ©gager la nazi ??? https:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>je dois faire un oral en anglais sur Hitler et...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@ambrassmoi_ A Ã§a ð¤ de ressembler Ã  la cr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Par curiositÃ©, jâai voulu Ã©couter les dis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@tical10 @ExBarcaFan74523 @ConflitsFrance Mdr ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet label\n",
       "1  Elle ne devait pas dÃ©gager la nazi ??? https:...     1\n",
       "2  je dois faire un oral en anglais sur Hitler et...     0\n",
       "3  @ambrassmoi_ A Ã§a ð¤ de ressembler Ã  la cr...     0\n",
       "4  \"Par curiositÃ©, jâai voulu Ã©couter les dis...     0\n",
       "5  @tical10 @ExBarcaFan74523 @ConflitsFrance Mdr ...     1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the categorical variables\n",
    "df[categorical].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet    0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values in categorical variables\n",
    "df[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"0. Moi\\n                                                                                                                                                    4\n",
      "@flo__lfc @leo2saucisson @ActuFoot_ CrÃ¨ve                                                                                                                   2\n",
      "\"ððððð\\n                                                                                                                                      2\n",
      "\"\"\"A Bright Room Called Day\"\", la cinglante leÃ§on d'histoire de Tony Kushner. Ma critique dans Les SoirÃ©es de Paris â¶ï¸Â https://t.co/Ydl1PqJv7AÂ \\n    2\n",
      "\"En ce moment sur LFM #CrÃ¨ve-cÅur#Mauvais Åil et toujours Le Morning sur LFM de 07h Ã  10h du Lundi au Vendredi.\\n                                        2\n",
      "                                                                                                                                                            ..\n",
      "de toutes faÃ§ons, c'est une mascarade. personne n'est mort et le masque sert Ã  rien #nazi #GiletsJaunes                                                    1\n",
      "Ah je crÃ¨ve de froid                                                                                                                                        1\n",
      "@Weatherboy_fr @ButchCoolidge11 @DamienRieu Bah oui câest les franÃ§ais qui ont Ã©lus hitler câest connuâ¦ gÃ©nie !                                     1\n",
      "@mormach Pitoyable cette information dirigÃ©e !!! On en crÃ¨ve !!!                                                                                           1\n",
      "Sal Noir https://t.co/9Sr7f4Mzr5 https://t.co/53F9rBpJsf                                                                                                     1\n",
      "Name: tweet, Length: 2847, dtype: int64\n",
      "0    1929\n",
      "1     927\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# view frequency counts of values in categorical variables\n",
    "for var in categorical: \n",
    "    print(df[var].value_counts())\n",
    "\n",
    "# 0        1929\n",
    "# 1         927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"0. Moi\\n                                                                                                                                                    0.001401\n",
      "@flo__lfc @leo2saucisson @ActuFoot_ CrÃ¨ve                                                                                                                   0.000700\n",
      "\"ððððð\\n                                                                                                                                      0.000700\n",
      "\"\"\"A Bright Room Called Day\"\", la cinglante leÃ§on d'histoire de Tony Kushner. Ma critique dans Les SoirÃ©es de Paris â¶ï¸Â https://t.co/Ydl1PqJv7AÂ \\n    0.000700\n",
      "\"En ce moment sur LFM #CrÃ¨ve-cÅur#Mauvais Åil et toujours Le Morning sur LFM de 07h Ã  10h du Lundi au Vendredi.\\n                                        0.000700\n",
      "                                                                                                                                                               ...   \n",
      "de toutes faÃ§ons, c'est une mascarade. personne n'est mort et le masque sert Ã  rien #nazi #GiletsJaunes                                                    0.000350\n",
      "Ah je crÃ¨ve de froid                                                                                                                                        0.000350\n",
      "@Weatherboy_fr @ButchCoolidge11 @DamienRieu Bah oui câest les franÃ§ais qui ont Ã©lus hitler câest connuâ¦ gÃ©nie !                                     0.000350\n",
      "@mormach Pitoyable cette information dirigÃ©e !!! On en crÃ¨ve !!!                                                                                           0.000350\n",
      "Sal Noir https://t.co/9Sr7f4Mzr5 https://t.co/53F9rBpJsf                                                                                                     0.000350\n",
      "Name: tweet, Length: 2847, dtype: float64\n",
      "0    0.67542\n",
      "1    0.32458\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# view frequency distribution of categorical variables\n",
    "for var in categorical:   \n",
    "    print(df[var].value_counts()/float(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1929\n",
       "1     927\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # check frequency distribution of values in label variable\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       elle ne devait pas dã©gager la nazi ??? https:...\n",
       "2       je dois faire un oral en anglais sur hitler et...\n",
       "3       @ambrassmoi_ a ã§a ð¤ de ressembler ã  la cr...\n",
       "4       \"par curiositã©, jâai voulu ã©couter les dis...\n",
       "5       @tical10 @exbarcafan74523 @conflitsfrance mdr ...\n",
       "                              ...                        \n",
       "2852                           kevin câest un sal noir.\n",
       "2853    bougnoule bamboula sale arabe , elle est belle...\n",
       "2854      @b_mahrezz tu viens de le traiter de sal noir ?\n",
       "2855                     sal noir https://t.co/jraabftel8\n",
       "2856    sal noir https://t.co/9sr7f4mzr5 https://t.co/...\n",
       "Name: lowered_tweet, Length: 2856, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lowercase all words\n",
    "def make_lower(a_string):\n",
    "    return a_string.lower()\n",
    "\n",
    "df['lowered_tweet'] = df['tweet'].apply(make_lower)\n",
    "df['lowered_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       elle ne devait pas dãgager la nazi  httpstcona...\n",
       "2       je dois faire un oral en anglais sur hitler et...\n",
       "3       ambrassmoi_ a ãa ð de ressembler ã  la croix n...\n",
       "4       par curiositã jâai voulu ãcouter les discours ...\n",
       "5       tical10 exbarcafan74523 conflitsfrance mdr deu...\n",
       "                              ...                        \n",
       "2852                              kevin câest un sal noir\n",
       "2853    bougnoule bamboula sale arabe  elle est belle ...\n",
       "2854        b_mahrezz tu viens de le traiter de sal noir \n",
       "2855                          sal noir httpstcojraabftel8\n",
       "2856       sal noir httpstco9sr7f4mzr5 httpstco53f9rbpjsf\n",
       "Name: remove_punctuation, Length: 2856, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all punctuation\n",
    "def remove_punctuation(a_string):    \n",
    "    a_string = re.sub(r'[^\\w\\s]','',a_string)\n",
    "    return a_string\n",
    "\n",
    "df['remove_punctuation'] = df['lowered_tweet'].apply(remove_punctuation)\n",
    "df['remove_punctuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['remove_punctuation'] = df['remove_punctuation'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>lowered_tweet</th>\n",
       "      <th>remove_punctuation</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elle ne devait pas dÃ©gager la nazi ??? https:...</td>\n",
       "      <td>1</td>\n",
       "      <td>elle ne devait pas dã©gager la nazi ??? https:...</td>\n",
       "      <td>elle ne devait pas dãgager la nazi  httpstcona...</td>\n",
       "      <td>[elle, ne, devait, pas, dãgager, la, nazi, htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>je dois faire un oral en anglais sur Hitler et...</td>\n",
       "      <td>0</td>\n",
       "      <td>je dois faire un oral en anglais sur hitler et...</td>\n",
       "      <td>je dois faire un oral en anglais sur hitler et...</td>\n",
       "      <td>[je, dois, faire, un, oral, en, anglais, sur, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@ambrassmoi_ A Ã§a ð¤ de ressembler Ã  la cr...</td>\n",
       "      <td>0</td>\n",
       "      <td>@ambrassmoi_ a ã§a ð¤ de ressembler ã  la cr...</td>\n",
       "      <td>ambrassmoi_ a ãa ð de ressembler ã  la croix n...</td>\n",
       "      <td>[ambrassmoi, _, a, ãa, ð, de, ressembler, ã, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Par curiositÃ©, jâai voulu Ã©couter les dis...</td>\n",
       "      <td>0</td>\n",
       "      <td>\"par curiositã©, jâai voulu ã©couter les dis...</td>\n",
       "      <td>par curiositã jâai voulu ãcouter les discours ...</td>\n",
       "      <td>[par, curiositã, jâai, voulu, ãcouter, les, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@tical10 @ExBarcaFan74523 @ConflitsFrance Mdr ...</td>\n",
       "      <td>1</td>\n",
       "      <td>@tical10 @exbarcafan74523 @conflitsfrance mdr ...</td>\n",
       "      <td>tical10 exbarcafan74523 conflitsfrance mdr deu...</td>\n",
       "      <td>[tical, 10, exbarcafan, 74523, conflitsfrance,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet label  \\\n",
       "1  Elle ne devait pas dÃ©gager la nazi ??? https:...     1   \n",
       "2  je dois faire un oral en anglais sur Hitler et...     0   \n",
       "3  @ambrassmoi_ A Ã§a ð¤ de ressembler Ã  la cr...     0   \n",
       "4  \"Par curiositÃ©, jâai voulu Ã©couter les dis...     0   \n",
       "5  @tical10 @ExBarcaFan74523 @ConflitsFrance Mdr ...     1   \n",
       "\n",
       "                                       lowered_tweet  \\\n",
       "1  elle ne devait pas dã©gager la nazi ??? https:...   \n",
       "2  je dois faire un oral en anglais sur hitler et...   \n",
       "3  @ambrassmoi_ a ã§a ð¤ de ressembler ã  la cr...   \n",
       "4  \"par curiositã©, jâai voulu ã©couter les dis...   \n",
       "5  @tical10 @exbarcafan74523 @conflitsfrance mdr ...   \n",
       "\n",
       "                                  remove_punctuation  \\\n",
       "1  elle ne devait pas dãgager la nazi  httpstcona...   \n",
       "2  je dois faire un oral en anglais sur hitler et...   \n",
       "3  ambrassmoi_ a ãa ð de ressembler ã  la croix n...   \n",
       "4  par curiositã jâai voulu ãcouter les discours ...   \n",
       "5  tical10 exbarcafan74523 conflitsfrance mdr deu...   \n",
       "\n",
       "                                     tokenized_tweet  \n",
       "1  [elle, ne, devait, pas, dãgager, la, nazi, htt...  \n",
       "2  [je, dois, faire, un, oral, en, anglais, sur, ...  \n",
       "3  [ambrassmoi, _, a, ãa, ð, de, ressembler, ã, l...  \n",
       "4  [par, curiositã, jâai, voulu, ãcouter, les, di...  \n",
       "5  [tical, 10, exbarcafan, 74523, conflitsfrance,...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.TweetTokenizer()\n",
    "# keeps hashtags together, subset of word_tokenize\n",
    "\n",
    "df['tokenized_tweet'] = df['remove_punctuation'].apply(tokenizer.tokenize)\n",
    "df.head()\n",
    "\n",
    "# https://stackoverflow.com/questions/48363461/passing-a-pandas-dataframe-column-to-an-nltk-tokenizer\n",
    "#df['tweet'] = df['tweet'].apply(nltk.TweetTokenizer)\n",
    "# this overwrote what i did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tweet'] = df['tweet'].astype(pd.StringDtype())\n",
    "\n",
    "df['tokenized_tweet'] = df['tokenized_tweet'].astype(str)\n",
    "# must feed str into vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile a RE into regex object to look for occurrences of same pattern inside target strings w/o rewriting it\n",
    "# ALF adds to data file\n",
    "def add_lexical_features(dataframe):\n",
    "    urls = re.compile(r\"http\")\n",
    "    ats = re.compile(r\"@[a-zA-Z.]*\")\n",
    "    hashtags = re.compile(r\"#[a-zA-Z]*\")\n",
    "    letters = re.compile(r\"[a-zA-Z]\")\n",
    "    caps = re.compile(r\"[A-Z]\")\n",
    "    fancy = [\";\",'\\\"','(','<<']\n",
    "\n",
    "    nbr_characters = [len(s) for s in df.tweet]\n",
    "    df['nbr_characters'] = pd.DataFrame(nbr_characters, index=df.index)\n",
    "\n",
    "    nbr_words = [len(s.split()) for s in df.tweet] # to update after cleaning\n",
    "    df['nbr_words'] = pd.Series(nbr_words, index=df.index)\n",
    "\n",
    "    nbr_ats = [len(ats.findall(text)) for text in df.tweet]\n",
    "    df['nbr_ats'] = pd.Series(nbr_ats, index=df.index)\n",
    "\n",
    "    nbr_hashtags = [len(hashtags.findall(text)) for text in df.tweet]\n",
    "    df['nbr_hashtags'] = pd.Series(nbr_hashtags, index=df.index)\n",
    "\n",
    "    nbr_urls = [len(urls.findall(text)) for text in df.tweet]\n",
    "    df['nbr_urls'] = pd.Series(nbr_urls, index=df.index)\n",
    "\n",
    "    nbr_letters = [len(letters.findall(text)) for text in df.tweet]\n",
    "    df['nbr_letters'] = pd.Series(nbr_letters, index=df.index)\n",
    "\n",
    "    nbr_caps = [len(caps.findall(text)) for text in df.tweet]\n",
    "    df['nbr_caps'] = pd.Series(nbr_caps, index=df.index)\n",
    "\n",
    "    nbr_fancy = [sum(1 for c in text if c in fancy) for text in df.tweet]\n",
    "    df['nbr_fancy'] = pd.Series(nbr_fancy, index=df.index)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tokenized_tweet'] # list of tokenized tweets\n",
    "y = df['label'] # list of labels of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "# randomizes data in splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "# indicating random state ensures data split will be same\n",
    "#https://stackoverflow.com/questions/28064634/random-state-pseudo-random-number-in-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2284,), (572,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of X_train and X_test\n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "# ((2284, 10), (572, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2284,), (572,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape\n",
    "# ((2284,), (572,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data types in X_train\n",
    "\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_fr = nltk.corpus.stopwords.words(\"french\")\n",
    "# https://stackoverflow.com/questions/57359982/remove-stopwords-in-french-and-english-in-tfidfvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# # can't run LR or MNB (models) until vectorize (or bow)\n",
    "\n",
    "# # Initialize our vectorizer (featurizer, turning text to numbers)\n",
    "# #only looking at numbers produced, creating features\n",
    "# # takes each tweet turning into feature vector, each column corresponds to word\n",
    "# # vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words=stopwords_fr) # empty model\n",
    "\n",
    "# # fit vectorizer w/ train data (training data)\n",
    "# # This makes your vocab matrix, learns words in vocab\n",
    "# vectorizer.fit(X_train)\n",
    "\n",
    "# # This transforms your documents into vectors (numbers).\n",
    "# X_train = vectorizer.transform(X_train)\n",
    "# X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score of model with n-gram range of (1, 1): 0.476\n",
      "F-1 score of model with n-gram range of (1, 2): 0.4686\n",
      "F-1 score of model with n-gram range of (1, 3): 0.4686\n",
      "F-1 score of model with n-gram range of (1, 4): 0.4634\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# train model with different n-gram ranges\n",
    "for N in range(1,5):\n",
    "    \n",
    "    # convert training data to bag of words\n",
    "    range_vectorizer = TfidfVectorizer(ngram_range=(1,N), stop_words=stopwords_fr)\n",
    "    range_X_train = range_vectorizer.fit_transform(X_train)\n",
    "# can't run more than once- got attribute lower error b/c was receiving matrix when run once, not right datatype\n",
    "    range_X_test = range_vectorizer.transform(X_test)\n",
    "    \n",
    "    # train model and generate predictions\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(range_X_train, y_train)\n",
    "    y_pred = clf.predict(range_X_test)\n",
    "    \n",
    "    # compute f-1 score\n",
    "    score = np.round(f1_score(y_test, y_pred, average='macro'),4)\n",
    "    print('F-1 score of model with n-gram range of {}: {}'.format((1,N), score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to test and evaluate models. \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_and_eval_model(model, X, y):\n",
    "    print(\"Running report for model:\", model) \n",
    "    \n",
    "    # Fit our model.\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    # predicting results based on testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Make predicted probabilites of our testing data\n",
    "#     y_pred_proba = model.predict_proba(X_test)\n",
    "# https://stackoverflow.com/questions/15111408/how-does-sklearn-svm-svcs-function-predict-proba-work-internally\n",
    "\n",
    "    # Get the evaluation metrics \n",
    "    print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "    # y_test are true class labels and y_pred are predicted class labels in test-set, don't use train data\n",
    "    \n",
    "    # f1-score is always lower than accuracy measures, they embed precision and recall into their computation\n",
    "#     print('F1 Score: %.4f' % f1_score(y_test, y_pred, pos_label='0'))\n",
    "    # https://datascience.stackexchange.com/questions/54436/valueerror-pos-label-1-is-not-a-valid-label-arrayn-y-dtype-u1\n",
    "    # data not much, low f1score; cross fold validation to generate data articificially\n",
    "    f1score = f1_score(y_test, y_pred, average='macro')\n",
    "    print('macro F-1 score : {}'.format(np.round(f1score,5)))\n",
    "    # macro avg f1score = unweighted mean of all the per-class F1 scores, treats all classes equally important\n",
    "    # weighted-averaged F1 score is calculated by taking the mean of all per-class F1 scores while considering each class’s support\n",
    "    ## ‘weight’ refers to proportion of each class’s support relative to the sum of all support values\n",
    "    # https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f\n",
    "    \n",
    "    # compare the train-set and test-set accuracy to check for overfitting \n",
    "    y_pred_train = model.predict(X_train) \n",
    "    # overlearned training data = training accuracy higher (but not over so b/c not 99)\n",
    "    ## close enough to \"possibly\" overfit but not dramatic difference --> trust model accuracy score (minor point)\n",
    "    print('Training set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))\n",
    "    print('Test set accuracy score: {:.4f}'.format(model.score(X_test, y_test)))\n",
    "\n",
    "    # Print the classification report. \n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    \n",
    "        \n",
    "    # Plot the confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(13,13))\n",
    "    \n",
    "    metrics.plot_confusion_matrix(estimator=model, \n",
    "                                  X=X_test, \n",
    "                                  y_true=y_test, \n",
    "                                  ax=ax)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print('Confusion matrix\\n\\n', cm)\n",
    "    print('\\nTrue Positives(TP) = ', cm[0,0])\n",
    "    print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
    "    print('\\nFalse Positives(FP) = ', cm[0,1])\n",
    "    print('\\nFalse Negatives(FN) = ', cm[1,0])\n",
    "\n",
    "#     # visualize confusion matrix with seaborn heatmap\n",
    "#     cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n",
    "#                                  index=['Predict Positive:1', 'Predict Negative:0'])\n",
    "#     sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n",
    "    \n",
    "    \n",
    "    # print classification accuracy, same as test set score & model accuracy score\n",
    "    classification_accuracy = (cm[0,0] + cm[1,1]) / float(cm[0,0] + cm[1,1] + cm[0,1] + cm[1,0])\n",
    "    print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))\n",
    "    \n",
    "    # print classification error\n",
    "    classification_error = (cm[0,1] + cm[1,0]) / float(cm[0,0] + cm[1,1] + cm[0,1] + cm[1,0])\n",
    "    print('Classification error : {0:0.4f}'.format(classification_error))\n",
    "\n",
    "    # Return the fitted model. \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "build_and_eval_model(lr, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "lsvc = svm.LinearSVC()\n",
    "build_and_eval_model(lsvc, X, y)\n",
    "# class is imbalanced but not a lot of data is impacting 0 detection better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha=.05)\n",
    "build_and_eval_model(mnb, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC()\n",
    "build_and_eval_model(svc, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "build_and_eval_model(rf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  a Wilcoxon test, as Vanetik and Mimoun 2022 do, to find the p-value,"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
