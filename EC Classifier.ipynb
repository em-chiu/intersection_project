{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/em-chiu/intersection_project/blob/main/EC%20Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSgFvHzhSPNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe6a4603-759f-483e-b192-e891f58f83e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import urllib.request\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn import preprocessing\n",
        "import string\n",
        "nltk.download('averaged_perceptron_tagger') # to solve pos feature extract issue\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "non_clickbait_url = \"http://www.cs.columbia.edu/~sarahita/CL/non_clickbait_data.txt\"\n",
        "clickbait_url = \"http://www.cs.columbia.edu/~sarahita/CL/clickbait_data.txt\"\n",
        "\n",
        "# read url .txt file into string \"data\"\n",
        "def get_data(url):\n",
        "  data = urllib.request.urlopen(url).read().decode('utf-8')\n",
        "  return data\n",
        "\n",
        "non_clickbait_data = get_data(non_clickbait_url)\n",
        "clickbait_data = get_data(clickbait_url)"
      ],
      "metadata": {
        "id": "mQ8JQ1scSabd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine clickbait and non-clickbait data in a single list\n",
        "non_clickbait_headlines = non_clickbait_data.rstrip('\\n').split('\\n')\n",
        "clickbait_headlines = clickbait_data.rstrip('\\n').split('\\n')\n",
        "all_headlines = non_clickbait_headlines + clickbait_headlines"
      ],
      "metadata": {
        "id": "Hi_kgavhSdcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of corresponding labels\n",
        "non_cb_labels = [0] * len(non_clickbait_headlines)\n",
        "cb_labels = [1] * len(clickbait_headlines)\n",
        "all_labels = non_cb_labels + cb_labels"
      ],
      "metadata": {
        "id": "u4FcadFJSdXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features: bag of stop words\n",
        "def stop_words(texts):\n",
        "  bow = [] \n",
        "  eng_stopwords = stopwords.words('english')\n",
        "  for text in texts:      \n",
        "    counts = []\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    for sw in eng_stopwords:\n",
        "      sw_count = tokens.count(sw)\n",
        "      counts.append(sw_count)\n",
        "    bow.append(counts)\n",
        "  bow_np = np.array(bow).astype(float)\n",
        "  return bow_np"
      ],
      "metadata": {
        "id": "-QHb6D8zSdNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features\n",
        "stop_words_features = stop_words(all_headlines)\n",
        "\n"
      ],
      "metadata": {
        "id": "wZaIA5CnSdCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_features.shape"
      ],
      "metadata": {
        "id": "Yzb-5l6L04Qs",
        "outputId": "7e6b602f-b136-47cd-c99b-3a8e8824ce30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31998, 179)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert features and labels to numpy arrays\n",
        "X = stop_words_features # count vectorizer\n",
        "Y = np.array(all_labels)\n",
        "\n",
        "# run classifier using 10-fold cross validation\n",
        "# report mean accuracy \n",
        "\n",
        "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "DygmPc5pScsf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc014358-a82b-4d0f-9e18-9100634c0beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8735535323538606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features: POS tags\n",
        "def POS_tags(texts):\n",
        "  bow = [] # bag of word list initialized\n",
        "  POS = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS','CC','PRP','VB','VBG']\n",
        "  for text in texts:      \n",
        "      counts = []\n",
        "      tokens = nltk.word_tokenize(text.lower()) # tokenized text\n",
        "      tagged_words = nltk.pos_tag(tokens) # tag tokens, list of tuples\n",
        "      pos_tags = [x[1] for x in tagged_words] # 2nd element of tuple- POS of the tokens in list\n",
        "      # print(tokens,'\\n', tagged_words, '\\n', pos_tags, '\\n', POS) # look at variables to differentiate\n",
        "      for pos in POS:\n",
        "          pos_count = pos_tags.count(pos)\n",
        "          counts.append(pos_count)\n",
        "      bow.append(counts)\n",
        "  bow_np = np.array(bow).astype(float)\n",
        "  return bow_np"
      ],
      "metadata": {
        "id": "MBwzezqfwB3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features\n",
        "pos_features = POS_tags(all_headlines)"
      ],
      "metadata": {
        "id": "qdWWjViBwDWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecNDGqKNwE6a",
        "outputId": "0cc86dbc-db2d-4e36-c8cb-98efce4aa323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31998, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert features and labels to numpy arrays\n",
        "X = pos_features\n",
        "Y = np.array(all_labels)\n",
        "\n",
        "# run classifier using 10-fold cross validation\n",
        "# report mean accuracy \n",
        "\n",
        "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XkHeKTAwGs_",
        "outputId": "1b1c8ced-9d87-4b2c-eece-0f75fb805a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7672044584245077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk import ngrams"
      ],
      "metadata": {
        "id": "jVUr7eHcmxG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features: unigrams\n",
        "def unigrams_lex(texts):\n",
        "  unigrams = []\n",
        "  stop = set(stopwords.words('english'))\n",
        "  for headline in texts:\n",
        "    filtered_headlines = []\n",
        "    tokens = nltk.word_tokenize(headline)\n",
        "    filtered = [token for token in tokens if not (token in stop or token in string.punctuation)]\n",
        "    for token in tokens:\n",
        "        filtered_headlines.append(headline)\n",
        "        counter_obj = Counter(filtered_headlines)\n",
        "        top_thirty = counter_obj.most_common(30)\n",
        "        counts_thirty = [count[1] for count in top_thirty] # takes second element in tuple\n",
        "    unigrams.append(counts_thirty) # desired: counts for 30 most common unigrams in entire corpus (remove stopwords and punctuation for unigram count)\n",
        "  unigrams_np = np.array(unigrams, dtype=object)\n",
        "  return unigrams_np"
      ],
      "metadata": {
        "id": "s7TCOrz03cC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigrams_features = unigrams_lex(all_headlines)"
      ],
      "metadata": {
        "id": "iGaszEeYHEOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigrams_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr3vXEJuIlw0",
        "outputId": "5223b687-da1e-4748-fdbd-d877fd027f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31998, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert features and labels to numpy arrays\n",
        "X = unigrams_features\n",
        "Y = np.array(all_labels)\n",
        "\n",
        "# run classifier using 10-fold cross validation\n",
        "# report mean accuracy \n",
        "\n",
        "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nWTU8MQEO0B",
        "outputId": "8cdef537-325e-4ede-dbe9-f039b68f38fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.49996874023132226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_features = unigrams_lex(all_headlines)"
      ],
      "metadata": {
        "id": "k2-QIVoP6eZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmlMb9WkIY9N",
        "outputId": "417ee9ff-b454-49cb-9a0d-b65959daa7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31998, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features: punctuation mark in string.punctuation\n",
        "def count_puncs(texts):\n",
        "  bow = []\n",
        "  punctuation = string.punctuation\n",
        "  for text in texts:      \n",
        "      counts = []\n",
        "      tokens = nltk.word_tokenize(text.lower())\n",
        "      for punc in punctuation:\n",
        "          punc_count = tokens.count(punc)\n",
        "          counts.append(punc_count)\n",
        "      bow.append(counts)\n",
        "  bow_np = np.array(bow).astype(float) #converting bow list to numpy array list, converting to float\n",
        "  return bow_np"
      ],
      "metadata": {
        "id": "RowZqUk27jZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features\n",
        "punc_features = count_puncs(all_headlines)"
      ],
      "metadata": {
        "id": "lenXkS0c7nx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punc_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAjH2OPT7pQ2",
        "outputId": "5387afdd-8aaa-43f6-bc5a-a0e81083f428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31998, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert features and labels to numpy arrays\n",
        "X = punc_features\n",
        "Y = np.array(all_labels)\n",
        "\n",
        "# run classifier using 10-fold cross validation\n",
        "# report mean accuracy \n",
        "\n",
        "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dEU7Jg37qy8",
        "outputId": "7170a134-a0e3-44da-e081-c6921675104e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5012524812441388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change list to string\n",
        "headlines_str = '\\n'.join(all_headlines) # joining and puts on separate lines\n",
        "#space makes one big line of headlines"
      ],
      "metadata": {
        "id": "6ob1upGRHNLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use tokenized headlines instead of tokenizing each time\n",
        "tokenized_headlines = nltk.word_tokenize(headlines_str.lower())"
      ],
      "metadata": {
        "id": "xLb-fTQ2m8r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_avg_char_per_word(headlines):\n",
        "  total_length = sum(len(word) for word in headlines) #for word in headline.split())\n",
        "  num_words = len(tokenized_headlines)\n",
        "  return total_length/num_words"
      ],
      "metadata": {
        "id": "2mTzJc44Oax3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_uniquetotal_words(headline):\n",
        "  num_unique_words = len(set(headline)) # one headline at a time to get one unique number at the time of iteration\n",
        "  total_words = len(headline)\n",
        "  return num_unique_words/total_words"
      ],
      "metadata": {
        "id": "HnC_vdcQRMfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_num_words(headline):\n",
        "  return len(headline)"
      ],
      "metadata": {
        "id": "F4ilxsv_Qy0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_long_words(headline): # creating new item for list\n",
        "  for word in headline:\n",
        "    long_count = 0 # initialize w/ 0 for counting\n",
        "    if len(word) >= 6:\n",
        "      long_count += 1 #treating as a variable (answer is number, not list)\n",
        "  return long_count"
      ],
      "metadata": {
        "id": "jpwunff4NwQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_long_words(headline): # creating new item for list\n",
        "  long_count = 0 # initialize w/ 0 for counting\n",
        "  for word in headline:\n",
        "    if len(word) >= 6:\n",
        "      long_count += 1 #treating as a variable (answer is number, not list)\n",
        "  return long_count"
      ],
      "metadata": {
        "id": "BOX5hZSv_4cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_long_words(all_headlines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9tcETWz_7YZ",
        "outputId": "eebc3189-88e0-41dd-fa14-62a0653969e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31998"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features: complexity\n",
        "def complexity(headlines): # passing headlines, list of str (headlines)\n",
        "  complexity = [] # keeps track of list of headlines\n",
        "  for headline in headlines:\n",
        "    headline_features = [] # stores features for array- list affected by for loop, needed to append function values\n",
        "    tokens = nltk.word_tokenize(headline.lower()) #one headline\n",
        "    headline_features.append(get_avg_char_per_word(tokens)) #average character per word\n",
        "    headline_features.append(get_uniquetotal_words(tokens)) # unique/total words\n",
        "    headline_features.append(len(tokens)) # number of words\n",
        "    headline_features.append(get_long_words(tokens)) #long words\n",
        "    complexity.append(headline_features)\n",
        "  complexity_np = np.array(complexity).astype(float) # array = list of lists    \n",
        "  return complexity_np\n",
        "\n",
        "    # looking for 4 complexity features, go through/for every headline, extract 4 features- then save them\n",
        "#4 subfunctions, 1 for each complexity measure \n",
        "#pass a single headline to the function and return a number"
      ],
      "metadata": {
        "id": "j_JeMW7X6kKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complexity_features = complexity(all_headlines)"
      ],
      "metadata": {
        "id": "DL9enME4-xSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complexity_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN9i8QKs-6b-",
        "outputId": "55f69869-5888-4f7e-c394-36426960d1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31998, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert features and labels to numpy arrays\n",
        "X = complexity_features\n",
        "Y = np.array(all_labels)\n",
        "\n",
        "# run classifier using 10-fold cross validation\n",
        "# report mean accuracy \n",
        "\n",
        "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQaioE6sVjz_",
        "outputId": "1214f982-a464-404c-daf3-bfe6382e2ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5893177360112536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract own features\n",
        "def crypto_nft(texts):\n",
        "  bow = []\n",
        "  currency = ['crypto', 'nft', 'cryptocurrency', 'bitcoin', 'nfts', 'non-fungible','ethereum','blockchain', 'instant', 'profit', 'profits', 'money', 'rich', 'tips', 'investing']\n",
        "  for text in texts:      \n",
        "      counts = []\n",
        "      tokens = nltk.word_tokenize(text.lower())\n",
        "      for crypto in currency:\n",
        "          crypto_count = tokens.count(crypto)\n",
        "          counts.append(crypto_count)\n",
        "      bow.append(counts)\n",
        "  bow_np = np.array(bow).astype(float)\n",
        "  return bow_np"
      ],
      "metadata": {
        "id": "xE3_HfCe73kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features\n",
        "crypto_nft_features = crypto_nft(all_headlines)"
      ],
      "metadata": {
        "id": "2KPQxp-u75wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crypto_nft_features.shape"
      ],
      "metadata": {
        "id": "s3UG_4wJ760u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6981381-e2ab-4304-e814-9415e7b01095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31998, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert features and labels to numpy arrays\n",
        "X = crypto_nft_features\n",
        "Y = np.array(all_labels)\n",
        "\n",
        "# run classifier using 10-fold cross validation\n",
        "# report mean accuracy \n",
        "\n",
        "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "nGF7s4vb7703",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb9c96b-78c8-4506-da2a-cfdead02d0da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5011876465301657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract all features\n",
        "# concatenate all features\n",
        "a = stop_words_features\n",
        "b = pos_features\n",
        "c = unigrams_features\n",
        "d = punc_features\n",
        "e = complexity_features\n",
        "f = crypto_nft_features\n"
      ],
      "metadata": {
        "id": "yZ_vE-N08tlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# args = (a, b, c)\n",
        "# all_features = np.concatenate(args) # asks dimensions to be the same, 3 args limit"
      ],
      "metadata": {
        "id": "lhVQIGMPYwLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_features = np.column_stack((a, b, c, d, e, f))"
      ],
      "metadata": {
        "id": "H9x6OtAUYr2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert features and labels to numpy arrays\n",
        "X = all_features\n",
        "Y = np.array(all_labels)\n",
        "\n",
        "# run classifier using 10-fold cross validation\n",
        "# report mean accuracy \n",
        "\n",
        "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqHkhOkOZEjz",
        "outputId": "779273c5-26c4-4548-f0d0-19a9d3362e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8893370096123789\n"
          ]
        }
      ]
    }
  ]
}